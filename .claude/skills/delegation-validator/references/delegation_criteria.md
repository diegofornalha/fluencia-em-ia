# Delegation Validation Criteria

Detailed rubric for validating lesson plan coverage of Delegation competency from the AI Fluency Framework.

---

## Problem Awareness Criteria

**Core Question:** "Does the lesson teach students to define objectives and question IF they should use AI?"

### Score 10 (Exemplary) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Explicitly teaches students to ask "O que estou tentando realizar?"
- ‚úÖ Includes decision framework for WHEN to use AI vs when NOT to
- ‚úÖ Covers all 3 interaction modes (Automation, Augmentation, Agency) with examples
- ‚úÖ Has exercises where students practice questioning IF AI is appropriate
- ‚úÖ Shows examples where NOT using AI is the better choice

**Example indicators:**
- "Before using AI, ask yourself: Is this task appropriate for AI collaboration?"
- Exercise: "For each scenario, decide: no AI, automation, augmentation, or agency"
- Discussion: "When is doing it yourself better than using AI?"

---

### Score 7-9 (Strong) ‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Teaches objective-setting before AI use
- ‚úÖ Discusses when AI is/isn't appropriate
- ‚úÖ Mentions at least 2 interaction modes with explanations
- ‚ö†Ô∏è May lack comprehensive exercises

**Example indicators:**
- "First, define what you're trying to accomplish"
- "AI is good for X but not for Y"
- Mentions automation and augmentation

---

### Score 4-6 (Adequate) ‚≠ê‚≠ê
- ‚úÖ Mentions defining objectives
- ‚ö†Ô∏è Assumes AI will be used (doesn't question IF)
- ‚ö†Ô∏è May mention modes but without depth

**Example indicators:**
- "Set clear goals for your AI interaction"
- Assumes students will use AI for all tasks
- Mentions "different ways to use AI" but vaguely

---

### Score 0-3 (Weak) ‚≠ê
- ‚ùå No guidance on defining objectives
- ‚ùå Goes straight to "how to use AI"
- ‚ùå No discussion of appropriateness
- ‚ùå No mention of interaction modes

**Example indicators:**
- Starts with "Open ChatGPT and..."
- No discussion of when/whether to use AI
- Treats all AI use as equal

---

## Platform Awareness Criteria

**Core Question:** "Does the lesson teach students to compare AI tools and understand their capabilities/limitations?"

### Score 10 (Exemplary) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Compares 3+ AI tools with specific examples
- ‚úÖ Discusses capabilities AND limitations of each
- ‚úÖ Includes ethical/privacy considerations in tool selection
- ‚úÖ Has activity where students test different tools
- ‚úÖ Teaches trade-offs (free vs privacy, general vs specialized, etc.)

**Example indicators:**
- "Compare ChatGPT, Claude, and Copilot for code generation"
- "What can't this tool do? When does it fail?"
- "Consider privacy: does this tool store your data?"
- Exercise: "Test the same task on 2 tools and compare results"

---

### Score 7-9 (Strong) ‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Mentions 2-3 tools with some comparison
- ‚úÖ Discusses limitations (not just capabilities)
- ‚ö†Ô∏è May lack depth on ethical considerations

**Example indicators:**
- "ChatGPT is better for X, Claude for Y"
- "AI can hallucinate - always verify facts"
- Brief mention of privacy

---

### Score 4-6 (Adequate) ‚≠ê‚≠ê
- ‚úÖ Mentions multiple tools by name
- ‚ö†Ô∏è Doesn't compare them meaningfully
- ‚ö†Ô∏è Focuses on capabilities, not limitations

**Example indicators:**
- "You can use ChatGPT, Claude, or Gemini"
- Lists tools without explaining differences
- No discussion of when each is appropriate

---

### Score 0-3 (Weak) ‚≠ê
- ‚ùå Assumes one AI tool (usually ChatGPT)
- ‚ùå No comparison or alternatives mentioned
- ‚ùå No discussion of limitations

**Example indicators:**
- "Open ChatGPT" (as if it's the only option)
- No mention of other tools
- Assumes AI is always correct

---

## Task Delegation Criteria

**Core Question:** "Does the lesson teach students HOW to divide work strategically between human and AI?"

### Score 10 (Exemplary) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Provides specific strategies for dividing work (AI first, human first, iterative, etc.)
- ‚úÖ Includes examples of good vs bad delegation with justifications
- ‚úÖ Teaches what should stay human (critical thinking, ethics, creativity)
- ‚úÖ Has exercises where students plan delegation for projects
- ‚úÖ Shows how to adjust delegation when initial strategy doesn't work

**Example indicators:**
- "Strategy 1: Use AI for initial research, human for critical analysis"
- "Good delegation: AI generates 3 options, human decides. Bad: Human just accepts first AI output"
- Exercise: "Create delegation plan for essay: what AI does, what you do, in what order"
- "If AI output is weak, don't just regenerate - refine your delegation strategy"

---

### Score 7-9 (Strong) ‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Discusses division of work between human and AI
- ‚úÖ Provides some specific strategies
- ‚úÖ Mentions what humans should retain
- ‚ö†Ô∏è May lack comprehensive examples of good vs bad

**Example indicators:**
- "Use AI for brainstorming, you do final decisions"
- "Keep critical thinking as human responsibility"
- Some workflow examples

---

### Score 4-6 (Adequate) ‚≠ê‚≠ê
- ‚úÖ Generic advice on "use AI for X"
- ‚ö†Ô∏è Doesn't provide strategic framework
- ‚ö†Ô∏è Vague on what humans should do

**Example indicators:**
- "AI can help with research"
- "Let AI do the boring parts"
- No specific workflows or strategies

---

### Score 0-3 (Weak) ‚≠ê
- ‚ùå No guidance on dividing work
- ‚ùå Implies "give everything to AI"
- ‚ùå No discussion of human responsibilities

**Example indicators:**
- "Ask AI to do your homework"
- No discussion of collaboration
- AI presented as replacement, not tool

---

## Overall Score Interpretation

| Overall Score | Classification | Next Steps |
|---------------|----------------|------------|
| **8.0-10.0** | ‚úÖ Excellent - Ready to Pilot | Minor refinements, test with students |
| **6.0-7.9** | üü° Good - Needs Minor Improvements | Address specific gaps, then pilot |
| **4.0-5.9** | ‚ö†Ô∏è Adequate - Significant Gaps | Substantial revision before pilot |
| **0-3.9** | üî¥ Needs Major Work | Complete redesign required |

---

## Common Patterns in High-Scoring Lessons

### What Strong Lessons Have:
1. **Frameworks** - Decision trees, matrices, checklists
2. **Exercises** - Hands-on practice, not just reading
3. **Examples** - Good vs bad, with explanations
4. **Metacognition** - "Why did you choose that approach?"
5. **Iteration** - "What if that doesn't work?"

### What Weak Lessons Lack:
1. **Critical Questioning** - Assumes AI use, doesn't question it
2. **Comparison** - Only mentions one tool or approach
3. **Strategy** - Generic "use AI" without how/when/why
4. **Limitations** - Only shows AI's strengths
5. **Agency** - Students as passive consumers, not strategic planners

---

## Using This Rubric

### For Educators:
1. Read your lesson plan
2. For each criterion, find evidence in your plan
3. Score honestly (it's okay to score low - that's how you improve!)
4. Use gaps to revise systematically

### For Coaches (delegation-coach agent):
1. Use scores to identify strengths ("You're strong in X - what made that work?")
2. Use gaps to guide Socratic questions ("Platform Awareness scored 5 - what might be missing?")
3. Use recommendations as starting points, not prescriptions
4. Celebrate high scores, normalize low scores as growth opportunities

---

## Calibration Examples

See `example_plans.md` for annotated lesson plans showing what each score level looks like in practice.
